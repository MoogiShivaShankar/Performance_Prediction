# -*- coding: utf-8 -*-
"""Players_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ANp0R4hZaLXb3UYzr3LR6KJHEESGJLr
"""

# Importing Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_excel('world_cup_data.xlsx')

df.head()

# Create a performance metric
df['Performance_Score'] = df['Runs'] + (df['_4s'] * 0.5) + (df['_6s'] * 1) + (df['Strike_Rate'] * 0.2)

df.isnull().sum()

# Handle missing values
df['Maidens'] = df['Maidens'].fillna(0)
df['Wickets'] = df['Wickets'].fillna(0)
df['Economy'] = df['Economy'].fillna(df['Economy'].mean())
df['Overs'] = df['Overs'].fillna(0)

# Encode categorical variables
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Team'] = le.fit_transform(df['Team'])
df['Player_Name'] = le.fit_transform(df['Player_Name'])
df['Role'] = le.fit_transform(df['Role'])
df['Dismissal'] = le.fit_transform(df['Dismissal'])

# Select features
features = ['Team', 'Batting_Position', 'Runs', 'Balls', '_4s', '_6s', 'Strike_Rate',
            'Maidens', 'Wickets', 'Economy', 'Overs']

X = df[features]
y = df['Performance_Score']

from sklearn.model_selection import train_test_split
# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Check for any remaining NaN values
print("NaN values in X_train_scaled:", np.isnan(X_train_scaled).any())
print("NaN values in y_train:", np.isnan(y_train).any())

# If there are still NaN values, we can drop them
if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():
    mask = ~np.isnan(X_train_scaled).any(axis=1) & ~np.isnan(y_train)
    X_train_scaled = X_train_scaled[mask]
    y_train = y_train[mask]

"""# **Model Training**"""

# Train Random Forest model
from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

#Make predictionsfrom sklearn.impute import SimpleImputer

from sklearn.impute import SimpleImputer
# Create an imputer
imputer = SimpleImputer(strategy='mean')  # You can choose 'median' or 'most_frequent' as well

# Fit and transform the imputer on your training data
X_train_imputed = imputer.fit_transform(X_train_scaled)

# Transform your test data
X_test_imputed = imputer.transform(X_test_scaled)

# Now use the imputed data for prediction
y_pred = rf_model.predict(X_test_imputed)

# Make predictions
y_pred

import numpy as np

print("NaN in y_test:", np.isnan(y_test).any())
print("NaN in y_pred:", np.isnan(y_pred).any())

# Remove rows with NaN in y_pred
mask = ~np.isnan(y_pred)
y_test_clean = y_test[mask]
y_pred_clean = y_pred[mask]

import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

# Check for NaNs in y_test and y_pred
print("NaN in y_test:", np.isnan(y_test).any())
print("NaN in y_pred:", np.isnan(y_pred).any())

# Remove rows with NaN in either y_test or y_pred
mask = ~np.isnan(y_test) & ~np.isnan(y_pred)
y_test_clean = y_test[mask]
y_pred_clean = y_pred[mask]

# Check again to ensure no NaNs are present
print("NaN in y_test_clean:", np.isnan(y_test_clean).any())
print("NaN in y_pred_clean:", np.isnan(y_pred_clean).any())

# Recalculate metrics
if not np.isnan(y_test_clean).any() and not np.isnan(y_pred_clean).any():
    mse = mean_squared_error(y_test_clean, y_pred_clean)
    r2 = r2_score(y_test_clean, y_pred_clean)
    print(f"Mean Squared Error: {mse}")
    print(f"R2 Score: {r2}")
else:
    print("There are still NaN values present in the cleaned data.")

# Save the model and scaler
import joblib
joblib.dump(rf_model, 'cricket_performance_model.joblib')
joblib.dump(scaler, 'scaler.joblib')

"""New Data Prediction"""

df.values[0].reshape(1,-1)

# Select only numeric columns
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Scale only numeric data
scaler = StandardScaler()
scaled_numeric_data = scaler.fit_transform(df[numeric_columns])

# If you need to keep categorical data, you can combine it back after scaling
scaled_df = pd.DataFrame(scaled_numeric_data, columns=numeric_columns, index=df.index)
final_df = pd.concat([scaled_df, df.select_dtypes(exclude=['int64', 'float64'])], axis=1)

# To transform a single row (the first row in this case)
numeric_data = df[numeric_columns].iloc[0].values.reshape(1, -1)
scaled_single_row = scaler.transform(numeric_data)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np

# List of features the model was trained on (adjust this to match your model)
model_features = ['Runs', 'Balls', '_4s', '_6s', 'Strike_Rate', 'Maidens', 'Wickets', 'Economy', 'Overs', 'Batting_Position', 'Performance_Score']

# Create a copy of the DataFrame with only the required features
final_df = df[model_features].copy()

# Identify numeric and categorical columns
numeric_columns = final_df.select_dtypes(include=['int64', 'float64']).columns
categorical_columns = final_df.select_dtypes(include=['object']).columns

# Encode categorical variables
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    final_df[col] = le.fit_transform(final_df[col].astype(str))
    label_encoders[col] = le

# Impute missing values
imputer = SimpleImputer(strategy='mean')
final_df_imputed = pd.DataFrame(imputer.fit_transform(final_df), columns=final_df.columns)

# Scale numeric data if needed
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
final_df_scaled = pd.DataFrame(scaler.fit_transform(final_df_imputed), columns=final_df_imputed.columns)

# Now predict
try:
    predictions = rf_model.predict(final_df_scaled)
    print("Predictions:", predictions[:5])  # Print first 5 predictions
except Exception as e:
    print(f"An error occurred: {e}")
    print("Shape of input data:", final_df_scaled.shape)
    print("Columns in input data:", final_df_scaled.columns)
    print("Data types of columns:", final_df_scaled.dtypes)

# For a single prediction (e.g., first row)
single_row = final_df_scaled.iloc[0].values.reshape(1, -1)
try:
    single_prediction = rf_model.predict(single_row)
    print("Prediction for first row:", single_prediction)
except Exception as e:
    print(f"An error occurred for single prediction: {e}")

"""Pickle the Model File for deployment"""

import pickle
pickle.dump(rf_model,open('regmodel_1.pkl','wb'))
pickled_model = pickle.load(open('regmodel_1.pkl','rb'))
pickled_model.predict(final_df_scaled)
pickle.dump(scaler, open('scaler.pkl', 'wb'))

